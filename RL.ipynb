{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bru7jT8VLX6F"
      },
      "source": [
        "## **Tasks**\n",
        "* Implement Value Iteration, Policy Iteration that plan/learn to play 3x3 Tic-Tac-Toe game. Test agents against other rule-based agents that are provided. Play against all the agents including your own agents to test them.\n",
        "* A general framework for the game and agents is provided. Run each code cell below in order, when you see a <>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBek_YuBQzED"
      },
      "source": [
        "### Part 0: Environment: packages, constants, basic code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "ZGtHyRUXVZwm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pickle\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "# Constants for the game\n",
        "EMPTY = 0\n",
        "PLAYER_X = 1\n",
        "PLAYER_O = -1\n",
        "GAME_ROW, GAME_COL = 3, 3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8tNVHrqQzTK"
      },
      "source": [
        "#### 0.1: The ***Game*** class:\n",
        "\n",
        "play(): simulate one game\n",
        "\n",
        "*show_board* indicate whether the states are printed during play"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "DTW_W_wlQtve"
      },
      "outputs": [],
      "source": [
        "class Game:\n",
        "    \"\"\"\n",
        "    Define the tictactoe game. The function and variable names should be self explained.\n",
        "\n",
        "    @author: chenxy\n",
        "    \"\"\"\n",
        "    def __init__(self, player_x, player_o, show_board=False):\n",
        "        self.board = np.zeros((GAME_ROW, GAME_COL), dtype=int)\n",
        "        self.player_x = player_x\n",
        "        self.player_o = player_o\n",
        "        self.current_player = self.player_x\n",
        "        self.winner = None\n",
        "        self.show_board = show_board\n",
        "        self.turn = 0\n",
        "\n",
        "    def get_empty_positions(self):\n",
        "        return [(i, j) for i in range(GAME_ROW) for j in range(GAME_COL) if self.board[i, j] == EMPTY]\n",
        "\n",
        "    def is_winner(self, player):\n",
        "        symbol = player.symbol\n",
        "        for i in range(GAME_ROW):\n",
        "            if np.all(self.board[i, :] == symbol) or np.all(self.board[:, i] == symbol):\n",
        "                return True\n",
        "        if np.all(np.diag(self.board) == symbol) or np.all(np.diag(np.fliplr(self.board)) == symbol):\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def is_draw(self):\n",
        "        return np.all(self.board != EMPTY)\n",
        "\n",
        "    def make_move(self, position):\n",
        "        if self.board[position] != EMPTY:\n",
        "            # Don't raise an exception, just return indicating an invalid move\n",
        "            return False\n",
        "        self.board[position] = self.current_player.symbol\n",
        "        return True\n",
        "\n",
        "    def switch_player(self):\n",
        "        self.current_player = self.player_x if self.current_player == self.player_o else self.player_o\n",
        "\n",
        "    def get_hash(self, board=None):\n",
        "        if board is None:\n",
        "            board = self.board\n",
        "        return ','.join(str(int(elem)) for elem in board.flatten())\n",
        "\n",
        "    def reset(self):\n",
        "        self.__init__(self.player_x, self.player_o, self.show_board)\n",
        "\n",
        "    def is_terminal(self):\n",
        "        # Check for a win in rows, columns, and diagonals\n",
        "        for i in range(GAME_ROW):\n",
        "            if np.all(self.board[i] == self.current_player.symbol) or \\\n",
        "               np.all(self.board[:, i] == self.current_player.symbol):\n",
        "                return True\n",
        "        if np.all(np.diag(self.board) == self.current_player.symbol) or \\\n",
        "           np.all(np.diag(np.fliplr(self.board)) == self.current_player.symbol):\n",
        "            return True\n",
        "        # Check for a draw (no empty positions left)\n",
        "        if not np.any(self.board == EMPTY):\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def play(self):\n",
        "        self.reset()\n",
        "        while True:\n",
        "          position = self.current_player.move(self)\n",
        "          if not self.make_move(position):  # If move is invalid, skip to next turn\n",
        "            raise Exception(\"Something is wrong! No empty positions now!\")\n",
        "          self.turn+=1\n",
        "\n",
        "          if self.is_terminal():\n",
        "              if self.is_winner(self.current_player):\n",
        "                  self.winner = self.current_player.symbol\n",
        "                  print(f\"Player {self.current_player.symbol} wins!\")\n",
        "                  break  # Exit the loop immediately after a win\n",
        "\n",
        "              if self.is_draw():\n",
        "                  print(\"It's a draw!\")\n",
        "                  break  # Exit the loop immediately after a draw\n",
        "\n",
        "          if self.show_board:\n",
        "              print(f\"Turn {self.turn}: Player {self.current_player.symbol}\")\n",
        "              self.print_board()\n",
        "\n",
        "          self.switch_player()\n",
        "\n",
        "        if self.show_board:\n",
        "            self.print_board()  # Show the final board state\n",
        "\n",
        "    def print_board(self):\n",
        "        symbols = {EMPTY: ' ', PLAYER_X: 'X', PLAYER_O: 'O'}\n",
        "        for i in range(GAME_ROW):\n",
        "            print('|' + '|'.join(symbols[s] for s in self.board[i]) + '|')\n",
        "        print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-t2uA-1SRPx2"
      },
      "source": [
        "#### 0.2 Agent abstract class, all *agents* class inherit this one.\n",
        "* *RandomAgent*: perform random action\n",
        "* *AggressiveAgent*: choose the winning action\n",
        "* *DefensiveAgent*: stop opponent's winning action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "IfdUxuKtRYqW"
      },
      "outputs": [],
      "source": [
        "class Agent(ABC):\n",
        "    def __init__(self, symbol):\n",
        "        self.symbol = symbol\n",
        "        self.states_value = {}  # State values used by ValueIterationAgent\n",
        "\n",
        "    @abstractmethod\n",
        "    def move(self, game):\n",
        "        pass\n",
        "\n",
        "    def save_policy(self, file_name):\n",
        "        with open(file_name, 'wb') as f:\n",
        "            pickle.dump(self.states_value, f)\n",
        "\n",
        "    def load_policy(self, file_name):\n",
        "        with open(file_name, 'rb') as f:\n",
        "            self.states_value = pickle.load(f)\n",
        "\n",
        "class RandomAgent(Agent):\n",
        "    def move(self, game):\n",
        "        empty_cells = game.get_empty_positions()\n",
        "        if not empty_cells:\n",
        "            raise ValueError(\"No more moves left to play.\")\n",
        "        # Select a random move from the list of empty cells\n",
        "        return empty_cells[np.random.randint(len(empty_cells))]\n",
        "\n",
        "class AggressiveAgent(Agent):\n",
        "    def __init__(self, symbol):\n",
        "        super().__init__(symbol)\n",
        "\n",
        "    def move(self, game):\n",
        "        empty_positions = game.get_empty_positions()\n",
        "        board_copy = game.board.copy()\n",
        "        for position in empty_positions:\n",
        "            board_copy[position] = self.symbol\n",
        "            if game.is_winner(self):\n",
        "                return position\n",
        "            board_copy[position] = EMPTY  # Reset the position after check\n",
        "\n",
        "        # If no winning move found, return a random move\n",
        "        return empty_positions[np.random.choice(len(empty_positions))]\n",
        "\n",
        "class DefensiveAgent(Agent):\n",
        "    def __init__(self, symbol):\n",
        "        super().__init__(symbol)\n",
        "\n",
        "    def move(self, game):\n",
        "        opponent_symbol = PLAYER_O if self.symbol == PLAYER_X else PLAYER_X\n",
        "        empty_positions = game.get_empty_positions()\n",
        "        board_copy = game.board.copy()\n",
        "\n",
        "        # First, check if the opponent has a winning move and block it\n",
        "        for position in empty_positions:\n",
        "            board_copy[position] = opponent_symbol\n",
        "            if game.is_winner(self.__opponent()):\n",
        "                return position  # Block the opponent's winning move\n",
        "            board_copy[position] = EMPTY  # Reset the position after check\n",
        "\n",
        "        # If no blocking move is necessary, choose a random move\n",
        "        return empty_positions[np.random.choice(len(empty_positions))]\n",
        "\n",
        "    def __opponent(self):\n",
        "        # Private helper method to create a 'dummy' opponent with the opposite symbol\n",
        "        return RandomAgent(PLAYER_O if self.symbol == PLAYER_X else PLAYER_X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulB1esGpTBDe"
      },
      "source": [
        "#### 0.3: Useful and example functions:\n",
        "Some may never been called"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "YMSydRNZTD2p"
      },
      "outputs": [],
      "source": [
        "def get_hash(board=None):\n",
        "    return ','.join(str(int(elem)) for elem in board.flatten())\n",
        "\n",
        "\n",
        "def get_hashes(boards):\n",
        "    return [get_hash(board) for board in boards]\n",
        "\n",
        "def valid_state(board, symbol):\n",
        "\n",
        "    # check the board state is valid\n",
        "    if symbol==PLAYER_X:\n",
        "        return (np.sum(board==PLAYER_X)==np.sum(board==PLAYER_O))\n",
        "    if symbol==PLAYER_O:\n",
        "        return (np.sum(board==PLAYER_X)-np.sum(board==PLAYER_O)==1)\n",
        "\n",
        "def next_symbol(board):\n",
        "    if (np.sum(board==PLAYER_X)==np.sum(board==PLAYER_O)):\n",
        "        return PLAYER_X\n",
        "    else:\n",
        "        return PLAYER_O\n",
        "\n",
        "def is_terminal(board):\n",
        "    # Check for a win in rows, columns, and diagonals\n",
        "    for i in range(GAME_ROW):\n",
        "        if abs(np.sum(board[i, :])) == 3 or abs(np.sum(board[:, i])) == 3:\n",
        "            return True\n",
        "    if abs(sum(np.diag(board))) == 3 or abs(sum(np.diag(np.fliplr(board)))) == 3:\n",
        "        return True\n",
        "    # Check for a draw\n",
        "    if not np.any(board == EMPTY):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def get_reward(board, symbol):\n",
        "    # Define opponent's symbol\n",
        "    opponent_symbol = PLAYER_O if symbol == PLAYER_X else PLAYER_X\n",
        "\n",
        "    # Check for current player's win\n",
        "    for i in range(GAME_ROW):\n",
        "        if sum(board[i, :]) == GAME_ROW * symbol or sum(board[:, i]) == GAME_COL * symbol:\n",
        "            return 1\n",
        "    if sum(np.diag(board)) == GAME_ROW * symbol or sum(np.diag(np.fliplr(board))) == GAME_COL * symbol:\n",
        "        return 1\n",
        "\n",
        "    # Check for opponent's win\n",
        "    for i in range(GAME_ROW):\n",
        "        if sum(board[i, :]) == GAME_ROW * opponent_symbol or sum(board[:, i]) == GAME_COL * opponent_symbol:\n",
        "            return -1\n",
        "    if sum(np.diag(board)) == GAME_ROW * opponent_symbol or sum(np.diag(np.fliplr(board))) == GAME_COL * opponent_symbol:\n",
        "        return -1\n",
        "\n",
        "    # Check for a draw\n",
        "    if is_terminal(board):\n",
        "        return 0\n",
        "\n",
        "    # For non-terminal states, the immediate reward is 0.\n",
        "    return 0\n",
        "\n",
        "\n",
        "def get_empty_positions(board):\n",
        "    return [(i, j) for i in range(GAME_ROW) for j in range(GAME_COL) if board[i, j] == EMPTY]\n",
        "\n",
        "def generate_next_boardstates(board, symbol):\n",
        "\n",
        "    # check the board state is valid\n",
        "    if symbol==PLAYER_X:\n",
        "        assert(np.sum(board==PLAYER_X)==np.sum(board==PLAYER_O))\n",
        "    if symbol==PLAYER_O:\n",
        "        assert(np.sum(board==PLAYER_X)-np.sum(board==PLAYER_O)==1)\n",
        "\n",
        "    # generate all next board states\n",
        "    all_empty_positions = get_empty_positions(board)\n",
        "    all_boards = np.tile(board, (len(all_empty_positions), 1, 1))\n",
        "    all_indices = np.concatenate(\n",
        "        [\n",
        "            np.expand_dims(np.arange(len(all_empty_positions)), axis=1),\n",
        "            np.array(all_empty_positions)\n",
        "        ], axis=1\n",
        "        )\n",
        "    all_boards[all_indices[:,0], all_indices[:,1], all_indices[:,2]] = symbol\n",
        "    all_boards = np.split(all_boards, all_boards.shape[0], axis=0)\n",
        "    return [np.squeeze(board) for board in all_boards]\n",
        "\n",
        "def generate_all_states(board, all_states=None, stop_step=None):\n",
        "\n",
        "    # assert(valid_state(board, symbol)) # validate the states and symbol\n",
        "\n",
        "    if all_states is None:\n",
        "        # all_states = {}\n",
        "        boards = [board]\n",
        "        state_hashes = [get_hash(board)]\n",
        "        p0 = 0\n",
        "        p1 = 1\n",
        "        p2 = p1\n",
        "        step = 0\n",
        "        step_symbol = next_symbol(board)\n",
        "\n",
        "    while p0!=p1:\n",
        "      for p_state in range(p0,p1):\n",
        "          # print(step)\n",
        "          # print('p_state:', p_state)\n",
        "          # print('board:', boards[p_state])\n",
        "          # print('step_symbol:', step_symbol)\n",
        "          # print(p1-p0)\n",
        "          # print('------------------------')\n",
        "          if is_terminal(boards[p_state]):\n",
        "              continue\n",
        "          next_boards = generate_next_boardstates(boards[p_state], step_symbol)\n",
        "          next_hashes = get_hashes(next_boards)\n",
        "\n",
        "          # print(next_boards)\n",
        "\n",
        "          boards+=next_boards\n",
        "          state_hashes+=next_hashes\n",
        "          p2 += len(next_boards)\n",
        "\n",
        "      step_symbol = PLAYER_X if step_symbol == PLAYER_O else PLAYER_O\n",
        "      p0 = p1\n",
        "      p1 = p2\n",
        "      step+=1\n",
        "\n",
        "      if stop_step is not None:\n",
        "        if step == stop_step:\n",
        "          break\n",
        "    return dict(zip(state_hashes, boards))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtdFtIMCU9hA"
      },
      "source": [
        "#### 0.4: Fenerate all states using the function **generate_all_states** and save the states hush table in the local path for the future use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYr9IFtCVQOK",
        "outputId": "6fceef8c-1134-49b0-d597-32c10eb173a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In total,  5478 states\n"
          ]
        }
      ],
      "source": [
        "state_hush_fname = \"all_states_hush1.txt\"\n",
        "if os.path.isfile(state_hush_fname):\n",
        "    with open(state_hush_fname, 'rb') as f:\n",
        "        all_states = pickle.load(f)\n",
        "\n",
        "else:\n",
        "  temp_board = np.zeros((GAME_ROW, GAME_COL))\n",
        "  all_states = generate_all_states(temp_board)\n",
        "\n",
        "  # same hush table of all the states\n",
        "  with open(state_hush_fname, 'wb') as f:\n",
        "      pickle.dump(all_states, f)\n",
        "\n",
        "print(\"In total, \", len(all_states), \"states\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgZE3uSzeb5H"
      },
      "source": [
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7lNh8ZYYn9_"
      },
      "source": [
        "The former should perform **planning* using *value iteration and the latter should extract the policy and compute state values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "DLYzW35pbLSa"
      },
      "outputs": [],
      "source": [
        "class ValueIterationAgent(Agent):\n",
        "    def __init__(self, symbol, discount_factor=0.9, living_reward=-0.01):\n",
        "        super().__init__(symbol)\n",
        "        if 'all_states' in globals():\n",
        "            self.all_states = all_states\n",
        "        elif os.path.isfile(state_hush_fname):\n",
        "            with open(state_hush_fname, 'rb') as f:\n",
        "                self.all_states = pickle.load(f)\n",
        "        else:\n",
        "            raise Exception(\"No state hushes! Either run the code by order or create the state hush yourself\")\n",
        "\n",
        "        self.discount_factor = discount_factor  # Discount factor for future rewards\n",
        "        self.living_reward = living_reward  # Reward for living (negative for penalty)\n",
        "        self.value_function = {state: 0 for state in all_states.keys()}  # Initialize state values to 0\n",
        "\n",
        "        # self.all_states = all_states  # All possible states\n",
        "        self.win_reward=10.0;\n",
        "        self.lose_reward=-50.0;\n",
        "        self.living_reward=-1.00;\n",
        "        self.draw_reward=0.0;\n",
        "\n",
        "        self.policy = {}  # Initialize policy\n",
        "\n",
        "    def get_reward(self, board, symbol):\n",
        "        # Define opponent's symbol\n",
        "        opponent_symbol = PLAYER_O if symbol == PLAYER_X else PLAYER_X\n",
        "\n",
        "        # Check for current player's win\n",
        "        for i in range(GAME_ROW):\n",
        "            if sum(board[i, :]) == GAME_ROW * symbol or sum(board[:, i]) == GAME_COL * symbol:\n",
        "                return self.win_reward\n",
        "        if sum(np.diag(board)) == GAME_ROW * symbol or sum(np.diag(np.fliplr(board))) == GAME_COL * symbol:\n",
        "            return self.win_reward\n",
        "\n",
        "        # Check for opponent's win\n",
        "        # -- Your Code Here ---\n",
        "        for i in range(GAME_ROW):\n",
        "         if sum(board[i, :]) == GAME_ROW * opponent_symbol or sum(board[:, i]) == GAME_COL * opponent_symbol:\n",
        "            return self.lose_reward\n",
        "        if sum(np.diag(board)) == GAME_ROW * opponent_symbol or sum(np.diag(np.fliplr(board))) == GAME_COL * opponent_symbol:\n",
        "            return self.lose_reward\n",
        "\n",
        "\n",
        "        # Check for a draw\n",
        "        # -- Your Code Here --\n",
        "        if is_terminal(board):\n",
        "         return self.draw_reward\n",
        "\n",
        "        # For non-terminal states, the immediate reward is 0.\n",
        "        return 0\n",
        "\n",
        "    def train(self, threshold=0.00001):\n",
        "        # Value iteration algorithm\n",
        "        # -- Your Code Here--\n",
        "        while True:\n",
        "         convergence_rate = 0\n",
        "         for state_hash, board in self.all_states.items():\n",
        "          if not is_terminal(board):\n",
        "            old_value = self.value_function[state_hash]\n",
        "            optimal_value = float('-inf')\n",
        "\n",
        "            for action in get_empty_positions(board):\n",
        "              next_board = board.copy()\n",
        "              next_board[action] = self.symbol\n",
        "              #T(s,a,s')\n",
        "              next_state_hash = get_hash(next_board)\n",
        "              #R(s,a,s')\n",
        "              reward = self.get_reward(next_board, self.symbol)\n",
        "              # gamma * V(s')\n",
        "              discounted_next_value = self.discount_factor * self.value_function.get(next_state_hash,0)\n",
        "              action_value = reward + discounted_next_value\n",
        "              optimal_value = max(optimal_value,action_value)\n",
        "\n",
        "            self.value_function[state_hash] = optimal_value\n",
        "            convergence_rate = max(convergence_rate, abs(old_value - self.value_function[state_hash]))\n",
        "\n",
        "         if convergence_rate < threshold:\n",
        "           break\n",
        "        else:\n",
        "         self.value_function[state_hash] = self.get_reward(board, self.symbol)\n",
        "\n",
        "\n",
        "    def move(self, game):\n",
        "        # Return the move based on the current policy\n",
        "        current_state = game.get_hash()\n",
        "        if current_state in self.policy:\n",
        "            return self.policy[current_state]\n",
        "        else:\n",
        "            # In case current state is not in the policy, choose a random move\n",
        "            empty_positions = game.get_empty_positions()\n",
        "            return empty_positions[np.random.choice(len(empty_positions))]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mW5HT2e-XySW",
        "outputId": "e90db455-e9e0-433b-cd7c-55b20369a7ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Player 1 wins!\n"
          ]
        }
      ],
      "source": [
        "player_x = ValueIterationAgent(PLAYER_X)  # This is the value iteration agent\n",
        "player_o = RandomAgent(PLAYER_O)  # This is the random agent\n",
        "\n",
        "game = Game(player_x, player_o)\n",
        "# print(game.board)\n",
        "\n",
        "# Compute the policy using value iteration only for the value iteration agent\n",
        "player_x.train()  # We only need to compute this for player O\n",
        "\n",
        "# Play the game\n",
        "\n",
        "game.play()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Cdq_YzynN10",
        "outputId": "2f505701-5268-4ea3-8254-9c8966db8e9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Player -1 wins!\n",
            "Player 1 wins!\n",
            "Player -1 wins!\n",
            "Player 1 wins!\n",
            "Player 1 wins!\n"
          ]
        }
      ],
      "source": [
        "# Game 1:\n",
        "player_x = RandomAgent(PLAYER_X)\n",
        "player_o = ValueIterationAgent(PLAYER_O)\n",
        "\n",
        "game1 = Game(player_x, player_o)\n",
        "# print(game.board)\n",
        "\n",
        "player_o.train()\n",
        "\n",
        "game1.play()\n",
        "\n",
        "# Game 2:\n",
        "player_x = ValueIterationAgent(PLAYER_X)\n",
        "player_o = AggressiveAgent(PLAYER_O)\n",
        "\n",
        "game2 = Game(player_x, player_o)\n",
        "# print(game.board)\n",
        "\n",
        "player_x.train()\n",
        "\n",
        "game2.play()\n",
        "\n",
        "# Game 3:\n",
        "player_x = AggressiveAgent(PLAYER_X)\n",
        "player_o = ValueIterationAgent(PLAYER_O)\n",
        "\n",
        "game3 = Game(player_x, player_o)\n",
        "# print(game.board)\n",
        "\n",
        "player_o.train()\n",
        "\n",
        "game3.play()\n",
        "\n",
        "\n",
        "# Game 4:\n",
        "player_x = ValueIterationAgent(PLAYER_X)\n",
        "player_o = DefensiveAgent(PLAYER_O)\n",
        "\n",
        "game4 = Game(player_x, player_o)\n",
        "# print(game.board)\n",
        "\n",
        "player_x.train()\n",
        "\n",
        "game4.play()\n",
        "\n",
        "\n",
        "# Game 5:\n",
        "player_x = DefensiveAgent(PLAYER_X)\n",
        "player_o = ValueIterationAgent(PLAYER_O)\n",
        "\n",
        "game5 = Game(player_x, player_o)\n",
        "# print(game.board)\n",
        "\n",
        "player_o.train()\n",
        "\n",
        "game5.play()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 652
        },
        "id": "vX2Wr3twhT1g",
        "outputId": "f6aeed7e-5113-43cf-c32e-0a8745347c88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Game 0\n",
            "Player 1 wins!\n",
            "Player -1 wins!\n",
            "Player 1 wins!\n",
            "Player 1 wins!\n",
            "It's a draw!\n",
            "Player -1 wins!\n",
            "It's a draw!\n",
            "Player -1 wins!\n",
            "Player 1 wins!\n",
            "It's a draw!\n",
            "Player -1 wins!\n",
            "It's a draw!\n",
            "Player -1 wins!\n",
            "Player 1 wins!\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-ebc804f3b67d>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mValueIterationAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPLAYER_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m           \u001b[0mplayer_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32melif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mValueIterationAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPLAYER_O\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-69-3d15b83cbb13>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, threshold)\u001b[0m\n\u001b[1;32m     66\u001b[0m               \u001b[0mnext_state_hash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_hash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_board\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m               \u001b[0;31m#R(s,a,s')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m               \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_board\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m               \u001b[0;31m# gamma * V(s')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m               \u001b[0mdiscounted_next_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscount_factor\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state_hash\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-69-3d15b83cbb13>\u001b[0m in \u001b[0;36mget_reward\u001b[0;34m(self, board, symbol)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Check for current player's win\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGAME_ROW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mGAME_ROW\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msymbol\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mGAME_COL\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msymbol\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwin_reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mGAME_ROW\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msymbol\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfliplr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mGAME_COL\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msymbol\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Q1.2 1-5:\n",
        "\n",
        "game.show_board = False # Disable printing board, don't change\n",
        "iteration = 50\n",
        "round = 0\n",
        "\n",
        "\n",
        "\n",
        "# -- Your Code Here ---\n",
        "agent_combinations = [\n",
        "    (ValueIterationAgent(PLAYER_X), RandomAgent(PLAYER_O)),\n",
        "    (ValueIterationAgent(PLAYER_O), RandomAgent(PLAYER_X)),\n",
        "    (ValueIterationAgent(PLAYER_X), AggressiveAgent(PLAYER_O)),\n",
        "    (ValueIterationAgent(PLAYER_O), AggressiveAgent(PLAYER_X)),\n",
        "    (ValueIterationAgent(PLAYER_X), DefensiveAgent(PLAYER_O)),\n",
        "    (ValueIterationAgent(PLAYER_O), DefensiveAgent(PLAYER_X)),\n",
        "]\n",
        "\n",
        "for player_x, player_o in agent_combinations:\n",
        "\n",
        "  print(f\"Game {round}\")\n",
        "  round += 1\n",
        "  wins = 0\n",
        "  losts = 0\n",
        "  draw = 0\n",
        "  for _ in range(iteration):\n",
        "\n",
        "        game = Game(player_x, player_o)\n",
        "\n",
        "        if(ValueIterationAgent(PLAYER_X)):\n",
        "          player_x.train()\n",
        "\n",
        "        elif(ValueIterationAgent(PLAYER_O)):\n",
        "          player_o.train()\n",
        "\n",
        "        else:\n",
        "          pass\n",
        "\n",
        "        game.play()\n",
        "\n",
        "        if game.is_winner(player_x):\n",
        "            wins += 1\n",
        "        elif game.is_winner(player_o):\n",
        "            losts += 1\n",
        "        else:\n",
        "            draw += 1\n",
        "\n",
        "  print(f\"Wins:{wins}, Losts: {losts}, Draw:{draw}\")\n",
        "  print(f\"--------\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNeTi_PNjG0Q"
      },
      "source": [
        "\n",
        "\n",
        "Integrated Policy Iteration agent in PolicyIterationAgent by implementing the policy_evaluation(), policy_improvement(), train() methods. The policy_evaluation() method evaluates the current policy. The current values for the current policy should be stored in the provided policyValues map. The policy_improvement() method performs the Policy improvement step, and updates curPolicy. The train() method is the planning process, once done, an optimal policy is saved in the agent object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "04vRz54IkLY6"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "class PolicyIterationAgent(Agent):\n",
        "    def __init__(self, symbol, discount_factor=0.9, living_reward=-0.01):\n",
        "        super().__init__(symbol)\n",
        "\n",
        "        if 'all_states' in globals():\n",
        "            self.all_states = all_states\n",
        "        elif os.path.isfile(state_hush_fname):\n",
        "            with open(state_hush_fname, 'rb') as f:\n",
        "                self.all_states = pickle.load(f)\n",
        "        else:\n",
        "            raise Exception(\"No state hushes! Either run the code by order or create the state hush yourself\")\n",
        "\n",
        "        self.discount_factor = discount_factor  # Discount factor for future rewards\n",
        "        self.living_reward = living_reward  # Reward for living (negative for penalty)\n",
        "        self.value_function = {state: 0 for state in all_states.keys()}  # Initialize state values to 0\n",
        "\n",
        "        # self.all_states = all_states  # All possible states\n",
        "        self.win_reward=10.0;\n",
        "        self.lose_reward=-50.0;\n",
        "        self.living_reward=-1.00;\n",
        "        self.draw_reward=0.0;\n",
        "\n",
        "        self.all_states = all_states  # All possible states\n",
        "        self.discount_factor = discount_factor  # Discount factor for future rewards\n",
        "        # self.living_reward = living_reward  # Living reward (negative for penalty)\n",
        "        self.value_function = {state: 0 for state in all_states.keys()}  # Initialize state values to 0\n",
        "        # self.policy = {state: np.random.choice(get_empty_positions(board))\n",
        "        #                for state, board in all_states.items() if not is_terminal(board)}  # Random initial policy\n",
        "        # # Choosing a random tuple from the list of empty positions\n",
        "        self.policy = {state: random.choice(get_empty_positions(board))\n",
        "                   for state, board in all_states.items() if not is_terminal(board)}  # Random initial policy\n",
        "\n",
        "    def get_reward(self, board, symbol):\n",
        "        # -- Your Code Here ---\n",
        "         # Define opponent's symbol\n",
        "        opponent_symbol = PLAYER_O if symbol == PLAYER_X else PLAYER_X\n",
        "\n",
        "        # Check for current player's win\n",
        "        for i in range(GAME_ROW):\n",
        "            if sum(board[i, :]) == GAME_ROW * symbol or sum(board[:, i]) == GAME_COL * symbol:\n",
        "                return self.win_reward\n",
        "        if sum(np.diag(board)) == GAME_ROW * symbol or sum(np.diag(np.fliplr(board))) == GAME_COL * symbol:\n",
        "            return self.win_reward\n",
        "\n",
        "        # Check for opponent's win\n",
        "        # -- Your Code Here ---\n",
        "        for i in range(GAME_ROW):\n",
        "         if sum(board[i, :]) == GAME_ROW * opponent_symbol or sum(board[:, i]) == GAME_COL * opponent_symbol:\n",
        "            return self.lose_reward\n",
        "        if sum(np.diag(board)) == GAME_ROW * opponent_symbol or sum(np.diag(np.fliplr(board))) == GAME_COL * opponent_symbol:\n",
        "            return self.lose_reward\n",
        "\n",
        "\n",
        "        # Check for a draw\n",
        "        # -- Your Code Here --\n",
        "        if is_terminal(board):\n",
        "         return self.draw_reward\n",
        "\n",
        "        # For non-terminal states, the immediate reward is 0.\n",
        "        return 0\n",
        "\n",
        "    def policy_evaluation(self, threshold=0.0001):\n",
        "        # -- Your Code Here ---\n",
        "        while True:\n",
        "          convergence_rate = 0\n",
        "          for state_hash, board in self.all_states.items():\n",
        "\n",
        "            if not is_terminal(board):\n",
        "              old_value = self.value_function[state_hash]\n",
        "              action = self.policy[state_hash]\n",
        "\n",
        "              next_board = board.copy()\n",
        "              next_board[action] = self.symbol\n",
        "              next_state_hash = get_hash(next_board)\n",
        "#             #R(s,a,s')\n",
        "              reward = self.get_reward(next_board, self.symbol)\n",
        "              # gamma * V(s')\n",
        "              discounted_next_value = self.discount_factor * self.value_function.get(next_state_hash,0)\n",
        "\n",
        "              self.value_function[state_hash] = reward + discounted_next_value\n",
        "\n",
        "              convergence_rate = max(convergence_rate, abs(old_value - self.value_function[state_hash]))\n",
        "\n",
        "          if convergence_rate < threshold:\n",
        "            break\n",
        "\n",
        "          else:\n",
        "           self.value_function[state_hash] = self.get_reward(board, self.symbol)\n",
        "\n",
        "\n",
        "    def policy_improvement(self):\n",
        "        # -- Your Code Here --\n",
        "        policy_stable = True\n",
        "\n",
        "        for state_hash, board in self.all_states.items():\n",
        "\n",
        "          if is_terminal(board):\n",
        "           self.value_function[state_hash] = self.get_reward(board, self.symbol)\n",
        "          else:\n",
        "            old_action = self.policy[state_hash]\n",
        "\n",
        "            optimum_action = None\n",
        "            optimum_value = float('-inf')\n",
        "\n",
        "            for action in get_empty_positions(board):\n",
        "              next_board = board.copy()\n",
        "              next_board[action] = self.symbol\n",
        "              next_state_hash = get_hash(next_board)\n",
        "\n",
        "              #Sum [ R(s,a,s') + gamma * V(s')]\n",
        "              action_value = self.get_reward(next_board, self.symbol) + self.discount_factor * self.value_function.get(next_state_hash,0)\n",
        "\n",
        "              if action_value > optimum_value:\n",
        "                optimum_value = action_value\n",
        "                optimum_action = action\n",
        "\n",
        "            self.policy[state_hash] = optimum_action\n",
        "\n",
        "            if old_action != optimum_action:\n",
        "             policy_stable = False\n",
        "\n",
        "        return policy_stable\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        while True:\n",
        "            self.policy_evaluation()\n",
        "            if self.policy_improvement():\n",
        "                break\n",
        "\n",
        "    def move(self, game):\n",
        "        # Return the move based on the current policy\n",
        "        current_state = game.get_hash()\n",
        "        if current_state in self.policy:\n",
        "            return self.policy[current_state]\n",
        "        else:\n",
        "            # If the current state is not in the policy, choose a random move\n",
        "            empty_positions = game.get_empty_positions()\n",
        "            return empty_positions[np.random.choice(len(empty_positions))]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKzFj62Ml4oK",
        "outputId": "cad1c059-d323-47e1-d44e-cac2635f0268"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Player 1 wins!\n"
          ]
        }
      ],
      "source": [
        "player_o = RandomAgent(PLAYER_O)  # This is the random agent\n",
        "player_x = PolicyIterationAgent(PLAYER_X)  # This is the value iteration agent\n",
        "\n",
        "# player_x = ValueIterationAgent(PLAYER_X)  # This is the random agent\n",
        "# player_o = RandomAgent(PLAYER_O)  # This is the value iteration agent\n",
        "\n",
        "game = Game(player_x, player_o)\n",
        "# print(game.board)\n",
        "\n",
        "# Compute the policy using value iteration only for the value iteration agent\n",
        "player_x.train()  # We only need to compute this for player O\n",
        "\n",
        "# Play the game\n",
        "\n",
        "game.play()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3DVPAExmqVa"
      },
      "source": [
        "\n",
        "* Game1: RandomAgent \"X\" v.s. PolicyIterationAgent \"O\"\n",
        "* Game2: PolicyIterationAgent \"X\" v.s. AggressiveAgent \"O\"\n",
        "* Game3: PolicyIterationAgent \"O\" v.s. AggressiveAgent \"X\"\n",
        "* Game4: PolicyIterationAgent \"X\" v.s. DefensiveAgent \"O\"\n",
        "* Game 5: PolicyIterationAgent \"O\" v.s. DefensiveAgent \"X\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6mgJfvYnfMQ",
        "outputId": "24677927-b837-405c-c722-f36d66721b3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Player -1 wins!\n",
            "Player 1 wins!\n",
            "Player -1 wins!\n",
            "Player 1 wins!\n",
            "Player -1 wins!\n"
          ]
        }
      ],
      "source": [
        "# Game 1:\n",
        "player_x = RandomAgent(PLAYER_X)\n",
        "player_o = PolicyIterationAgent(PLAYER_O)\n",
        "\n",
        "game = Game(player_x, player_o)\n",
        "\n",
        "player_o.train()\n",
        "game.play()\n",
        "\n",
        "# Game 2:\n",
        "player_o = AggressiveAgent(PLAYER_O)\n",
        "player_x = PolicyIterationAgent(PLAYER_X)\n",
        "\n",
        "game = Game(player_x, player_o)\n",
        "\n",
        "player_x.train()\n",
        "game.play()\n",
        "\n",
        "# Game 3:\n",
        "player_x = AggressiveAgent(PLAYER_X)\n",
        "player_o = PolicyIterationAgent(PLAYER_O)\n",
        "\n",
        "game = Game(player_x, player_o)\n",
        "\n",
        "player_o.train()\n",
        "game.play()\n",
        "\n",
        "# Game 4:\n",
        "player_o = DefensiveAgent(PLAYER_O)\n",
        "player_x = PolicyIterationAgent(PLAYER_X)\n",
        "\n",
        "game = Game(player_x, player_o)\n",
        "\n",
        "player_x.train()\n",
        "game.play()\n",
        "\n",
        "# Game 5:\n",
        "player_x = DefensiveAgent(PLAYER_X)\n",
        "player_o = PolicyIterationAgent(PLAYER_O)\n",
        "\n",
        "game = Game(player_x, player_o)\n",
        "\n",
        "player_o.train()\n",
        "game.play()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "HCyXqiiznLei",
        "outputId": "a9e6e604-0e32-4fb6-c51d-af93a387f72d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Game 0\n",
            "Player 1 wins!\n",
            "Player 1 wins!\n",
            "Player 1 wins!\n",
            "Player 1 wins!\n",
            "Player 1 wins!\n",
            "Player 1 wins!\n",
            "Player 1 wins!\n",
            "Player 1 wins!\n",
            "Player 1 wins!\n",
            "Player 1 wins!\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-49cecefc2f08>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mValueIterationAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPLAYER_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m           \u001b[0mplayer_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32melif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mValueIterationAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPLAYER_O\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-74-127b256a1d31>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_improvement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-74-127b256a1d31>\u001b[0m in \u001b[0;36mpolicy_evaluation\u001b[0;34m(self, threshold)\u001b[0m\n\u001b[1;32m     68\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstate_hash\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboard\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_terminal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m               \u001b[0mold_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_function\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate_hash\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m               \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate_hash\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-67-75198cff14c0>\u001b[0m in \u001b[0;36mis_terminal\u001b[0;34m(board)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Check for a win in rows, columns, and diagonals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGAME_ROW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfliplr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2296\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2298\u001b[0;31m     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n\u001b[0m\u001b[1;32m   2299\u001b[0m                           initial=initial, where=where)\n\u001b[1;32m   2300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "game.show_board = False\n",
        "\n",
        "# -- Your Code Here ---\n",
        "iteration = 50\n",
        "round = 0\n",
        "\n",
        "agent_combinations = [\n",
        "    (PolicyIterationAgent(PLAYER_X), RandomAgent(PLAYER_O)),\n",
        "    (PolicyIterationAgent(PLAYER_O), RandomAgent(PLAYER_X)),\n",
        "    (PolicyIterationAgent(PLAYER_X), AggressiveAgent(PLAYER_O)),\n",
        "    (PolicyIterationAgent(PLAYER_O), AggressiveAgent(PLAYER_X)),\n",
        "    (PolicyIterationAgent(PLAYER_X), DefensiveAgent(PLAYER_O)),\n",
        "    (PolicyIterationAgent(PLAYER_O), DefensiveAgent(PLAYER_X)),\n",
        "]\n",
        "\n",
        "for player_x, player_o in agent_combinations:\n",
        "\n",
        "  print(f\"Game {round}\")\n",
        "  round += 1\n",
        "  wins = 0\n",
        "  losts = 0\n",
        "  draw = 0\n",
        "  for _ in range(iteration):\n",
        "\n",
        "        game = Game(player_x, player_o)\n",
        "\n",
        "        if(ValueIterationAgent(PLAYER_X)):\n",
        "          player_x.train()\n",
        "\n",
        "        elif(ValueIterationAgent(PLAYER_O)):\n",
        "          player_o.train()\n",
        "\n",
        "        else:\n",
        "          pass\n",
        "\n",
        "        game.play()\n",
        "\n",
        "        if game.is_winner(player_x):\n",
        "            wins += 1\n",
        "        elif game.is_winner(player_o):\n",
        "            losts += 1\n",
        "        else:\n",
        "            draw += 1\n",
        "\n",
        "  print(f\"Wins:{wins}, Losts: {losts}, Draw:{draw}\")\n",
        "  print(f\"--------\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zuio47AnyyF"
      },
      "source": [
        "\n",
        "\n",
        "A  QLearn agent in QLearnIterationAgent. The planning process is done in a plan() function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "XZkZXUbyy-UO"
      },
      "outputs": [],
      "source": [
        "\n",
        "class QLearningAgent(Agent):\n",
        "\n",
        "    def __init__(self, symbol, alpha=0.4, gamma=0.9, epsilon=0.1, living_penalty=-1):\n",
        "        super().__init__(symbol)\n",
        "        self.alpha = alpha  # Learning rate\n",
        "        self.gamma = gamma  # Discount factor\n",
        "        self.epsilon = epsilon  # Epsilon for the epsilon-greedy policy\n",
        "        self.Q = {}  # Initialize Q-table\n",
        "        self.living_penalty = living_penalty\n",
        "        self.win_reward = 10.0\n",
        "        self.lose_reward = -50.0\n",
        "        self.draw_reward = 0.0\n",
        "        self.initial_state = np.zeros((GAME_ROW, GAME_COL), dtype=int)  # Initialize the initial state\n",
        "        self.current_symbol = symbol  # The symbol of the current player\n",
        "\n",
        "    # Your code Here\n",
        "\n",
        "    def train(self, num_episodes= 100000):\n",
        "\n",
        "        for episode in range(num_episodes):\n",
        "          state = np.copy(self.initial_state)\n",
        "          complete =  False\n",
        "\n",
        "          while not complete:\n",
        "            state_hash = self.hash_state(state)\n",
        "            actions = self.get_available_actions(state)\n",
        "\n",
        "            if not actions:\n",
        "              break\n",
        "\n",
        "            selected_action = self.choose_action(state_hash,actions)\n",
        "            #print(f\"Episode: {episode}, Action: {selected_action}\")\n",
        "\n",
        "            new_state, reward, complete = self.make_move(state, selected_action, self.current_symbol)\n",
        "\n",
        "            #update Q-table\n",
        "            self.plan(state_hash, selected_action, reward, new_state)\n",
        "\n",
        "            state = new_state\n",
        "\n",
        "            #print(f\"Episode: {episode}, State:\\n{state}\")\n",
        "            #print(f\"Episode: {episode}, Complete: {complete}\")\n",
        "\n",
        "            # Ensure that your termination condition is being reached\n",
        "            if is_terminal(state):\n",
        "                #print(f\"Episode: {episode}, Terminal state reached.\")\n",
        "                break\n",
        "\n",
        "    def choose_action(self, state_hash, actions):\n",
        "    # Epsilon-greedy policy\n",
        "     if np.random.rand() < self.epsilon:\n",
        "        # Exploration\n",
        "        return random.choice(actions)\n",
        "     else:\n",
        "        # Exploitation\n",
        "        q_values = [self.Q.get((state_hash, action), 0) for action in actions]\n",
        "        q_max = max(q_values)\n",
        "        optimal_actions = [actions[i] for i in range(len(actions)) if q_values[i] == q_max ]\n",
        "        return random.choice(optimal_actions)\n",
        "\n",
        "\n",
        "\n",
        "    def plan(self, state, action, reward, new_state):\n",
        "\n",
        "        new_state_hash = self.hash_state(new_state)\n",
        "        max_future_q = max([self.Q.get((new_state_hash, next_action), 0) for next_action in self.get_available_actions(new_state)])\n",
        "\n",
        "        #Q(s,a) <- Q(s,a) + alpha * [(R(s,a,s') + gamma * max Q(s',a')) - Q(s,a)]\n",
        "        self.Q[(state, action)] = self.Q.get((state, action),0)  + self.alpha * ((reward + self.gamma * max_future_q) - self.Q.get((state, action),0))\n",
        "\n",
        "\n",
        "\n",
        "    def get_reward(self, new_state, symbol):\n",
        "      if is_terminal(new_state):\n",
        "        if get_reward(new_state, symbol) == -1:\n",
        "         return self.lose_reward\n",
        "        elif get_reward(new_state, symbol) == 1:\n",
        "          return self.win_reward\n",
        "        else:\n",
        "          return self.draw.reward\n",
        "      return self.living_penalty\n",
        "\n",
        "\n",
        "\n",
        "    def hash_state(self, state):\n",
        "        return str(state.reshape(GAME_ROW * GAME_COL))\n",
        "\n",
        "    def get_available_actions(self, state):\n",
        "        return [(i, j) for i in range(GAME_ROW) for j in range(GAME_COL) if state[i, j] == EMPTY]\n",
        "\n",
        "    def make_move(self, state, action, symbol):\n",
        "        new_state = np.array(state)\n",
        "        new_state[action] = symbol\n",
        "        reward = self.get_reward(new_state, symbol)\n",
        "        done = is_terminal(new_state)\n",
        "        return new_state, reward, done\n",
        "\n",
        "    def move(self, game):\n",
        "        # Extract the board from the Game object\n",
        "        board = game.board\n",
        "        state_hash = self.hash_state(board)\n",
        "        available_actions = self.get_available_actions(board)\n",
        "\n",
        "        if not available_actions:\n",
        "            raise ValueError(\"No available actions to make a move.\")\n",
        "\n",
        "        # Choose the best action based on the Q-table\n",
        "        action = self.choose_action(state_hash, available_actions)\n",
        "\n",
        "        # Convert action to the format expected by Game's make_move method (e.g., (row, col))\n",
        "        return action\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCfNHR8joh8W",
        "outputId": "8faa5083-9378-4781-eeaf-3c9e6d6803ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Player 1 wins!\n"
          ]
        }
      ],
      "source": [
        "# Initialize the QLearningAgent with its symbol (X or O)\n",
        "q_learning_agent = QLearningAgent(PLAYER_X)\n",
        "\n",
        "# Assume there is a random agent for the opponent\n",
        "random_agent = RandomAgent(PLAYER_O)\n",
        "\n",
        "# Initialize the game environment with both agents\n",
        "game = Game(q_learning_agent, random_agent)\n",
        "\n",
        "# Train the QLearningAgent with a function that simulates playing the game\n",
        "# The train function would need to be implemented to simulate games within the agent\n",
        "q_learning_agent.train()\n",
        "\n",
        "# Use the game's play function to start playing\n",
        "game.play()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nClnharq80A"
      },
      "source": [
        "\n",
        "* Game1: RandomAgent \"X\" v.s. QLearnAgent \"O\"\n",
        "* Game2: QLearnAgent \"X\" v.s. AggressiveAgent \"O\"\n",
        "* Game3: QLearnAgent \"O\" v.s. AggressiveAgent \"X\"\n",
        "* Game4: QLearnAgent \"X\" v.s. DefensiveAgent \"O\"\n",
        "* Game 5: QLearnAgent \"O\" v.s. DefensiveAgent \"X\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4WgO8SHrPgG",
        "outputId": "8c8b129a-25ea-4369-c1d4-3590eb93737f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Player -1 wins!\n",
            "Player 1 wins!\n",
            "Player -1 wins!\n",
            "Player 1 wins!\n",
            "Player -1 wins!\n"
          ]
        }
      ],
      "source": [
        "# Game 1:\n",
        "q_learning_agent = QLearningAgent(PLAYER_O)\n",
        "opposing_agent = RandomAgent(PLAYER_X)\n",
        "\n",
        "game1 = Game(q_learning_agent, opposing_agent)\n",
        "q_learning_agent.train()\n",
        "\n",
        "game1.play()\n",
        "\n",
        "# Game 2:\n",
        "q_learning_agent = QLearningAgent(PLAYER_X)\n",
        "opposing_agent = AggressiveAgent(PLAYER_O)\n",
        "\n",
        "game2 = Game(q_learning_agent, opposing_agent)\n",
        "q_learning_agent.train()\n",
        "\n",
        "game2.play()\n",
        "\n",
        "# Game 3:\n",
        "q_learning_agent = QLearningAgent(PLAYER_O)\n",
        "opposing_agent = AggressiveAgent(PLAYER_X)\n",
        "\n",
        "game3 = Game(q_learning_agent, opposing_agent)\n",
        "q_learning_agent.train()\n",
        "\n",
        "game3.play()\n",
        "\n",
        "\n",
        "# Game 4:\n",
        "q_learning_agent = QLearningAgent(PLAYER_X)\n",
        "opposing_agent = DefensiveAgent(PLAYER_O)\n",
        "\n",
        "game4 = Game(q_learning_agent, opposing_agent)\n",
        "q_learning_agent.train()\n",
        "\n",
        "game4.play()\n",
        "\n",
        "\n",
        "# Game 5:\n",
        "q_learning_agent = QLearningAgent(PLAYER_O)\n",
        "opposing_agent = DefensiveAgent(PLAYER_X)\n",
        "\n",
        "game = Game(q_learning_agent, opposing_agent)\n",
        "q_learning_agent.train()\n",
        "\n",
        "game.play()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZAp7JyorMac"
      },
      "source": [
        "Repeated the games (Game 0-5) above 50 rounds each Game. Using QLearnAgent, prints out number of *wins*, *losts* and *draw* "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnNjSEt2xEo4",
        "outputId": "8883a19b-14c5-4684-a316-60d36d668c0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Game 0\n",
            "Player 1 wins!\n",
            "Player 1 wins!\n",
            "Player 1 wins!\n",
            "Player 1 wins!\n",
            "Player 1 wins!\n",
            "Player -1 wins!\n",
            "Player -1 wins!\n",
            "Player -1 wins!\n",
            "Player 1 wins!\n",
            "Player 1 wins!\n",
            "Player 1 wins!\n",
            "Player 1 wins!\n",
            "Player 1 wins!\n",
            "Player 1 wins!\n",
            "Player -1 wins!\n",
            "Player 1 wins!\n",
            "Player 1 wins!\n",
            "Player 1 wins!\n",
            "Player 1 wins!\n",
            "Player 1 wins!\n",
            "Player -1 wins!\n",
            "Player -1 wins!\n",
            "Player 1 wins!\n",
            "Player 1 wins!\n",
            "Player -1 wins!\n",
            "It's a draw!\n",
            "It's a draw!\n",
            "Player -1 wins!\n",
            "It's a draw!\n",
            "Player -1 wins!\n",
            "Player -1 wins!\n",
            "Player -1 wins!\n",
            "Player 1 wins!\n",
            "Player -1 wins!\n",
            "Player 1 wins!\n"
          ]
        }
      ],
      "source": [
        "game.show_board = False\n",
        "\n",
        "# -- Your Code Here --\n",
        "iteration = 50\n",
        "round = 0\n",
        "\n",
        "agent_combinations = [\n",
        "    (QLearningAgent(PLAYER_X), RandomAgent(PLAYER_O)),\n",
        "    (QLearningAgent(PLAYER_O), RandomAgent(PLAYER_X), ),\n",
        "    (QLearningAgent(PLAYER_X), AggressiveAgent(PLAYER_O)),\n",
        "    (QLearningAgent(PLAYER_O), AggressiveAgent(PLAYER_X), ),\n",
        "    (QLearningAgent(PLAYER_X), DefensiveAgent(PLAYER_O)),\n",
        "    (QLearningAgent(PLAYER_O), DefensiveAgent(PLAYER_X), ),\n",
        "]\n",
        "\n",
        "for player_x, player_o in agent_combinations:\n",
        "\n",
        "  print(f\"Game {round}\")\n",
        "  round += 1\n",
        "  wins = 0\n",
        "  losts = 0\n",
        "  draw = 0\n",
        "  for _ in range(iteration):\n",
        "\n",
        "        game = Game(player_x, player_o)\n",
        "\n",
        "        if(QLearningAgent(PLAYER_X)):\n",
        "          player_x.train()\n",
        "\n",
        "        elif(QLearningAgent(PLAYER_O)):\n",
        "          player_o.train()\n",
        "\n",
        "        else:\n",
        "          pass\n",
        "\n",
        "        game.play()\n",
        "\n",
        "        if game.is_winner(player_x):\n",
        "            wins += 1\n",
        "        elif game.is_winner(player_o):\n",
        "            losts += 1\n",
        "        else:\n",
        "            draw += 1\n",
        "\n",
        "  print(f\"Wins:{wins}, Losts: {losts}, Draw:{draw}\")\n",
        "  print(f\"--------\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
